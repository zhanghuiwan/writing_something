{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 生成张量\n",
    "\n",
    "### 1.1 使用torch.tensor()或者torch.Tensor()来生成张量。\n",
    "两者主要区别为\n",
    "- `torch.Tensor(data)`：直接调用`Tensor`构造函数会默认创建一个浮点型张量 (`torch.float32`)。即使输入的数据是整数列表或元组，生成的张量元素也会被转换为浮点数。\n",
    "- `torch.tensor(data)`：此函数会根据输入数据的类型推断出张量的类型。如果输入的是整数列表，则生成的张量将具有整数类型（例如`torch.int64`），而浮点数列表则会生成浮点类型的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 生成一个2维的，形状为2*3的张量, 元素全为0, 数据类型为32位浮点型\n",
    "a = torch.Tensor(2,3)\n",
    "# 生成一个1维的，形状为2的张量, 数据类型为32位浮点型\n",
    "b = torch.Tensor([2,3])\n",
    "\n",
    "# 生成一个2维的，形状为2*3的张量,数据类型为torch.int64\n",
    "c = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "# 生成一个2维的，形状为2*3的张量，数据类型为64位浮点型, 存储在cpu上, 并且需要梯度\n",
    "d = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64, device='cpu', requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 一些基本的张量\n",
    "| 函数                                  | 描述                                                                                                           |\n",
    "|-------------------------------------|--------------------------------------------------------------------------------------------------------------|\n",
    "| `torch.zeros(3, 3)`                 | 创建一个形状为 (3, 3) 的全零张量。所有元素都初始化为 0。                                                        |\n",
    "| `torch.ones(3, 3)`                  | 创建一个形状为 (3, 3) 的全一张量。所有元素都初始化为 1。                                                        |\n",
    "| `torch.eye(3, 3)`                   | 创建一个形状为 (3, 3) 的单位矩阵（对角线元素为1，其余元素为0）。                                               |\n",
    "| `torch.rand(3, 3)`                  | 创建一个形状为 (3, 3) 的张量，元素是从均匀分布 $ \\text{Uniform}(0, 1) $ 中抽取的随机数，值在 [0, 1) 区间内等概率出现。 |\n",
    "| `torch.randn(3, 3)`                 | 创建一个形状为 (3, 3) 的张量，元素是从标准正态分布 $ \\mathcal{N}(0, 1) $ 中抽取的随机数，均值为 0，方差为 1。       |\n",
    "| `torch.empty(3, 4)`                 | 创建一个形状为 (3, 4) 的未初始化张量。它不会将内存设置为任何特定值，因此创建速度较快，但内容是未定义的。           |\n",
    "| `torch.rand_like(h, dtype=torch.float)` | 创建一个与现有张量 `h` 大小相同的新张量，元素是从均匀分布 $ \\text{Uniform}(0, 1) $ 中抽取的随机数，并指定数据类型为 `float`。 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5189, 0.5840, 0.4035, 0.1913],\n",
       "        [0.8590, 0.7277, 0.5265, 0.8071],\n",
       "        [0.9355, 0.1644, 0.9047, 0.2824]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  = torch.rand(3,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 一些简单函数\n",
    "### 2.1 基本函数\n",
    "\n",
    "| 函数                                 | 作用                                                                 |\n",
    "|------------------------------------|--------------------------------------------------------------------|\n",
    "| `A.dim()`                          | 获取A的维度                                                        |\n",
    "| `A.item()`                         | 将Tensor转化为基本数据类型，注意Tensor中只有一个元素的时候才可以使用，一般用于在Tensor中取出数值 |\n",
    "| `A.numpy()`                        | 将Tensor转化为Numpy类型                                             |\n",
    "| `A.size()`                         | 查看尺寸                                                             |\n",
    "| `A.shape`                          | 查看尺寸                                                             |\n",
    "| `A.permute(0, 2, 3, 1)`            | 假设A是一个四维张量，其形状为 (N, C, H, W)将这个张量的形状改变为 (N, H, W, C)                                                           |\n",
    "| `A.dtype`                          | 查看A中元素的类型（张量中的所有元素都具有相同的类型）                                                         |\n",
    "| `A.view()`                         | 重构张量尺寸，类似于Numpy中的reshape                                   |\n",
    "| `A.transpose(0, 1)`                | 行列交换                                                             |\n",
    "| `A[1:], A[-1, -1]=100`             | 切片操作，类似Numpy中的切片                                           |\n",
    "| `A.zero_()`                        | 归零化                                                               |\n",
    "| `torch.stack((A, B), dim=-1)`      | 拼接，升维                                                           |\n",
    "| `torch.diag(A)`                    | 取A对角线元素形成一个一维向量                                         |\n",
    "| `torch.diag_embed(A)`              | 将一维向量放到对角线中，其余数值为0的Tensor                               |\n",
    "| `torch.flatten(A, start_dim=1)`    |将A从第1个维度展平，如果输入形状是 (N, H, W, C)，输出将会是形状为 (N, H*W*C) |\n",
    "\n",
    "#### `torch.cat` \n",
    "是 PyTorch 库中的一个函数，用于沿着给定的维度连接（拼接）一系列张量。这个函数要求所有输入张量除了指定的连接维度外，在其他所有维度上的大小都必须匹配。\n",
    "\n",
    "```python\n",
    "torch.cat(tensors, dim=0, *, out=None) -> Tensor\n",
    "```\n",
    "\n",
    "- `tensors`：这是一个张量序列（如列表或元组），这些张量将被连接。\n",
    "- `dim`：这是指明在哪个维度上进行连接的参数，默认是 `0`，即最外层的维度。\n",
    "- `out`：可选参数，输出张量可以被指定到这个参数中。\n",
    "\n",
    "\n",
    "沿着第一个维度（行）连接\n",
    "\n",
    "假设我们有两个形状为 `(2, 3)` 的张量：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "result = torch.cat((tensor1, tensor2), dim=0)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "这将输出一个形状为 `(4, 3)` 的新张量：\n",
    "\n",
    "```\n",
    "tensor([[ 1,  2,  3],\n",
    "        [ 4,  5,  6],\n",
    "        [ 7,  8,  9],\n",
    "        [10, 11, 12]])\n",
    "```\n",
    "\n",
    "沿着第二个维度（列）连接\n",
    "\n",
    "如果我们想要沿着第二个维度（列）来连接两个形状为 `(2, 3)` 的张量：\n",
    "\n",
    "```python\n",
    "result = torch.cat((tensor1, tensor2), dim=1)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "这将输出一个形状为 `(2, 6)` 的新张量：\n",
    "\n",
    "```\n",
    "tensor([[ 1,  2,  3,  7,  8,  9],\n",
    "        [ 4,  5,  6, 10, 11, 12]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获取Tensor的值：item() :  4.199999809265137\n",
      "获取Tensor的个数：numel() :  3\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([2.3, 4.2, 3])\n",
    "print(\"获取Tensor的值：item() : \", v[1].item())\n",
    "\n",
    "print(\"获取Tensor的个数：numel() : \", v.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 基本运算\n",
    "\n",
    "y = x + y 系统将取消引⽤ y 的地址，指向新地址。y指向地址不同。\n",
    "y[:] = x + y 和 y += x 系统不再分配新空间，y指向的地址相同\n",
    "\n",
    "| 函数                                 | 作用                                                                 |\n",
    "|------------------------------------|--------------------------------------------------------------------|\n",
    "| `torch.abs(A)`                     | 计算绝对值                                                         |\n",
    "| `torch.add(A, B)`                  | 相加，A和B既可以是Tensor也可以是标量                                               |\n",
    "| `torch.clamp(A, min, max)`         | 裁剪，A中的数据若小于min或大于max，则变成min或max，即保证范围在[min, max]               |\n",
    "| `torch.div(A, B)`                  | 相除，A / B，A和B既可以是Tensor也可以是标量                                           |\n",
    "| `torch.mul(A, B)`                  | 点乘，A * B，A和B既可以是Tensor也可以是标量                                           |\n",
    "| `torch.pow(A, n)`                  | 求幂，A的n次方                                                     |\n",
    "| `torch.mm(A, B.T)`                 | 矩阵叉乘，注意与`torch.mul`之间的区别                                       |\n",
    "| `torch.mv(A, B)`                   | 矩阵与向量相乘，A是矩阵，B是向量，这里的B需不需要转置都是可以的                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. cuda相关用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1+cpu\n",
      "None\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试GPU环境是否可使用\n",
    "print(torch.__version__) # pytorch版本\n",
    "print(torch.version.cuda) # cuda版本\n",
    "print(torch.cuda.is_available()) # 查看cuda是否可用\n",
    " \n",
    "#使用GPU or CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "# 判断某个对象是在什么环境中运行的\n",
    "a.device\n",
    " \n",
    "# 将对象的环境设置为device环境\n",
    "A = a.to(device)\n",
    " \n",
    "# 将对象环境设置为CPU\n",
    "A.cpu().device\n",
    " \n",
    "# 若一个没有环境的对象与另外一个有环境a对象进行交流,则环境全变成环境a\n",
    "a.to(device)\n",
    " \n",
    "# cuda环境下tensor不能直接转化为numpy类型,必须要先转化到cpu环境中\n",
    "a.cpu().numpy()\n",
    " \n",
    "# 创建CUDA型的tensor\n",
    "torch.tensor([1,2], device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 梯度\n",
    "\n",
    "### 4.1 求导\n",
    "- 对于非标量的输出 y，比如形状为 (N,) 的向量，并且你想要计算输入 x 对应的梯度，那么你需要指定一个形状与 y 相同的权重张量 gradient。这个权重张量代表了每个输出元素对最终目标（通常是损失函数）的贡献。\n",
    "- retain_graph=True：用于在调用 backward() 后保持计算图不被释放，以便可以对同一前向传播的结果进行多次反向传播。\n",
    "- create_graph=True：用于在计算梯度时保留更高阶的梯度信息，使得可以计算二阶或更高阶导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量Tensor求导\n",
    "# 求 f(x) = a*x**2 + b*x + c 的导数\n",
    "x = torch.tensor(-2.0, requires_grad=True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(2.0)\n",
    "c = torch.tensor(3.0)\n",
    "y = a*torch.pow(x,2)+b*x+c # 计算y的值\n",
    "y.backward() # 计算y的梯度，实际上会计算此时关于每一个自变量x的偏导数，求得的结果会存储在每个自变量x的grad属性中\n",
    "dy_dx =x.grad\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.,  0.],\n",
       "        [ 2.,  4.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非标量Tensor求导\n",
    "# 求 f(x) = a*x**2 + b*x + c 的导数\n",
    "x = torch.tensor([[-2.0,-1.0],[0.0,1.0]], requires_grad=True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(2.0)\n",
    "c = torch.tensor(3.0)\n",
    "gradient=torch.tensor([[1.0,1.0],[1.0,1.0]]) # 与输出y的形状相同\n",
    "y = a*torch.pow(x,2)+b*x+c\n",
    "y.backward(gradient=gradient) \n",
    "dy_dx =x.grad\n",
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.) tensor(12.) tensor(24.)\n"
     ]
    }
   ],
   "source": [
    "#单个自变量求导\n",
    "# 求 f(x) = a*x**4 + b*x + c 的导数\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(2.0)\n",
    "c = torch.tensor(3.0)\n",
    "y = a * torch.pow(x, 4) + b * x + c\n",
    "#create_graph设置为True,允许创建更高阶级的导数\n",
    "#求一阶导\n",
    "dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "#求二阶导\n",
    "dy2_dx2 = torch.autograd.grad(dy_dx, x, create_graph=True)[0]\n",
    "#求三阶导\n",
    "dy3_dx3 = torch.autograd.grad(dy2_dx2, x)[0]\n",
    "print(dy_dx.data, dy2_dx2.data, dy3_dx3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) tensor(1.)\n",
      "tensor(3.) tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# 多个自变量求偏导\n",
    "x1 = torch.tensor(1.0, requires_grad=True)\n",
    "x2 = torch.tensor(2.0, requires_grad=True)\n",
    "y1 = x1 * x2\n",
    "y2 = x1 + x2\n",
    "#只有一个因变量,正常求偏导\n",
    "dy1_dx1, dy1_dx2 = torch.autograd.grad(outputs=y1, inputs=[x1, x2], retain_graph=True)\n",
    "print(dy1_dx1, dy1_dx2)\n",
    "# 若有多个因变量，则对于每个因变量,会将求偏导的结果加起来\n",
    "dy1_dx, dy2_dx = torch.autograd.grad(outputs=[y1, y2], inputs=[x1, x2])\n",
    "dy1_dx, dy2_dx\n",
    "print(dy1_dx, dy2_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "y= tensor(0.) ; x= tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "#例2-1-3 利用自动微分和优化器求最小值\n",
    "\n",
    " # f(x) = a*x**2 + b*x + c的最小值\n",
    "x = torch.tensor(0.0, requires_grad=True)  # x需要被求导\n",
    "a = torch.tensor(1.0)\n",
    "b = torch.tensor(-2.0)\n",
    "c = torch.tensor(1.0)\n",
    "optimizer = torch.optim.SGD(params=[x], lr=0.01)  #SGD为随机梯度下降\n",
    "print(optimizer)\n",
    " \n",
    "def f(x):\n",
    "    result = a * torch.pow(x, 2) + b * x + c\n",
    "    return (result)\n",
    " \n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()  #将模型的参数初始化为0\n",
    "    y = f(x)\n",
    "    y.backward()  #反向传播计算梯度\n",
    "    optimizer.step()  #更新所有的参数\n",
    "print(\"y=\", y.data, \";\", \"x=\", x.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Dataset and DataLoader\n",
    "\n",
    "\n",
    "`DataLoader` 是 PyTorch 中用于加载数据集并提供迭代器接口的类。以下是 `DataLoader` 的各个参数及其说明：\n",
    "\n",
    "```python\n",
    "DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    batch_sampler=None,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    timeout=0,\n",
    "    worker_init_fn=None,\n",
    "    multiprocessing_context=None,\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "| 参数               | 类型          | 默认值   | 描述                                                                                                           |\n",
    "|------------------|-------------|--------|------------------------------------------------------------------------------------------------------------|\n",
    "| `dataset`        | Dataset     | 必填项  | 数据集对象，决定数据从哪里读取以及如何读取。必须实现 `__len__` 和 `__getitem__` 方法。                                            |\n",
    "| `batch_size`     | int         | 1      | 每个批次（batch）的数据量大小。                                                                            |\n",
    "| `shuffle`        | bool        | False  | 如果为 `True`，则在每个 epoch 开始时打乱数据集。对于训练集通常设置为 `True`，而对于验证集或测试集则为 `False`。                    |\n",
    "| `sampler`        | Sampler     | None   | 自定义样本采样器。如果指定了 `sampler`，则忽略 `shuffle` 参数。常用的是 `RandomSampler` 和 `SequentialSampler`。                     |\n",
    "| `batch_sampler`  | BatchSampler| None   | 自定义批次采样器。如果指定了 `batch_sampler`，则忽略 `batch_size`、`shuffle` 和 `sampler` 参数。                                    |\n",
    "| `num_workers`    | int         | 0      | 加载数据时使用的子进程数。设置为 0 表示在主进程中加载数据。增加此参数可以加快数据加载速度，但也会占用更多内存和CPU资源。              |\n",
    "| `collate_fn`     | callable    | None   | 用于合并一个批次的数据样本的函数。默认情况下，PyTorch 会自动将张量堆叠在一起。如果你的数据结构更复杂，可能需要自定义此函数。            |\n",
    "| `pin_memory`     | bool        | False  | 如果为 `True`，则将数据加载到固定内存（pinned memory），以便更快地传输到GPU。适用于使用GPU进行训练的情况。                          |\n",
    "| `drop_last`      | bool        | False  | 如果数据集大小不能被 `batch_size` 整除，则最后一个不完整的批次是否丢弃。如果为 `True`，则最后一个批次会被丢弃。                      |\n",
    "| `timeout`        | numeric     | 0      | 等待数据加载的最大时间（以秒为单位）。如果超过此时间仍未获取到数据，则抛出异常。0 表示无超时限制。                                     |\n",
    "| `worker_init_fn` | callable    | None   | 每个工作进程初始化时调用的函数。可用于设置随机种子等操作。                                                   |\n",
    "| `multiprocessing_context` | context | None  | 设置多进程上下文，指定如何启动子进程。适用于高级用户，通常不需要更改。                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "class ImageDataset:\n",
    "    def __init__(self, raw_data):\n",
    "        self.raw_data = raw_data\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.raw_data[index]\n",
    "        return image, label\n",
    " \n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    " \n",
    "    def __iter__(self):\n",
    "        self.indexs = np.arange(len(self.dataset))\n",
    "        self.cursor = 0\n",
    "        np.random.shuffle(self.indexs)\n",
    "        return self\n",
    " \n",
    "    def __next__(self):\n",
    "        begin = self.cursor\n",
    "        end = self.cursor + self.batch_size\n",
    "        if end > len(self.dataset):\n",
    "            raise StopIteration()\n",
    "        self.cursor = end\n",
    "        batched_data = []\n",
    "        for index in self.indexs[begin:end]:\n",
    "            item = self.dataset[index]\n",
    "            batched_data.append(item)\n",
    "        return batched_data\n",
    " \n",
    "# Example usage\n",
    "images = [(f\"image{i}\", i) for i in range(100)]\n",
    "dataset = ImageDataset(images)\n",
    "loader = DataLoader(dataset, 5)\n",
    " \n",
    "for index, batched_data in enumerate(loader):\n",
    "    print(f\"Batch {index + 1}:\", batched_data)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 神经网络\n",
    "\n",
    "\n",
    "### 6.1 简单的线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "# 生成数据集，传入真实的w，b以及需要生成的数据集数量\n",
    "def synthetic_data(w, b, num_examples):  \n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    # 这里的-1 是一个特殊的维度值，它告诉PyTorch自动计算该维度的大小，使得总的元素数量与原张量相同。而 1 则明确指定了另一个维度的大小。\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "# 小批量读取数据集，batch_size为每次读取的大小，每次返回batch_size大小的数据\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "# 生成数据集\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "\n",
    "# 定义损失函数\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "# 定义优化算法\n",
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    # 告诉 PyTorch 不要跟踪计算图中的梯度信息。因为我们只是在这里更新参数，而不是计算新的梯度，所以不需要梯度跟踪。\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_() # 将梯度清0\n",
    "\n",
    "\n",
    "# 初始化模型参数，随机生成w，b\n",
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 定义参数\n",
    "lr = 0.03\n",
    "num_epochs = 4\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "batch_size = 10\n",
    "\n",
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        l = loss(net(X, w, b), y)  # X和y的小批量损失\n",
    "        # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        '''\n",
    "        l.sum().backward() 计算标量损失相对于模型参数 w 和 b 的梯度。这个操作会将梯度存储在对应的参数张量的 .grad 属性中。\n",
    "        在 PyTorch 中，backward() 方法实际上会沿着计算图反向传播梯度。这意味着它会计算损失相对于所有参与计算的\n",
    "        张量的梯度，并将这些梯度存储在相应的张量的 .grad 属性中。\n",
    "        '''\n",
    "        l.sum().backward()  \n",
    "        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad(): # 确保在评估期间不计算梯度\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n",
    "\n",
    "\n",
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 定义网络\n",
    "def net_mul_layer(X):\n",
    "    \"\"\"多层感知机的前向传播\"\"\"\n",
    "    H = torch.relu(X @ W1 + b1)  # 隐藏层\n",
    "    return H @ W2 + b2  # 输出层\n",
    "\n",
    "# 定义损失函数\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "\n",
    "# 初始化模型参数\n",
    "num_inputs, num_outputs, num_hiddens = 2, 1, 8\n",
    "\n",
    "W1 = nn.Parameter(torch.randn(\n",
    "    num_inputs, num_hiddens, requires_grad=True) * 0.01)\n",
    "b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))\n",
    "W2 = nn.Parameter(torch.randn(\n",
    "    num_hiddens, num_outputs, requires_grad=True) * 0.01)\n",
    "b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "# 定义训练参数\n",
    "lr = 0.03\n",
    "net = net_mul_layer\n",
    "loss = squared_loss\n",
    "num_epochs = 40\n",
    "batch_size = 10\n",
    "optimizer = torch.optim.SGD(params, lr=lr)\n",
    "\n",
    "\n",
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(net(X), y)  # 前向传播计算损失\n",
    "        l.mean().backward()  # 反向传播计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "    with torch.no_grad(): # 确保在评估期间不计算梯度\n",
    "        train_l = loss(net(features), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 定义损失函数\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "# 定义网络以及初始化参数\n",
    "net = nn.Sequential(nn.Linear(2, 8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "# 定义训练参数\n",
    "lr = 0.0003\n",
    "# net = net_mul_layer\n",
    "loss = squared_loss\n",
    "num_epochs = 400\n",
    "batch_size = 10\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(net(X), y)  # 前向传播计算损失\n",
    "        l.mean().backward()  # 反向传播计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "    with torch.no_grad(): # 确保在评估期间不计算梯度\n",
    "        train_l = loss(net(features), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 定义损失函数\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "# 定义网络以及初始化参数\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        # 定义层\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)  # 隐藏层\n",
    "        self.relu = nn.ReLU()  # 激活函数\n",
    "        self.output = nn.Linear(hidden_size, output_size)  # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义前向传播逻辑\n",
    "        x = self.hidden(x)  # 隐藏层\n",
    "        x = self.relu(x)    # 激活函数\n",
    "        x = self.output(x)  # 输出层\n",
    "        return x\n",
    "\n",
    "# 使用模型\n",
    "net = MLP(input_size=2, hidden_size=8, output_size=1)\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights)\n",
    "\n",
    "# 定义训练参数\n",
    "lr = 0.03\n",
    "# net = net_mul_layer\n",
    "loss = squared_loss\n",
    "num_epochs = 40\n",
    "batch_size = 10\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# 训练\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter(batch_size, features, labels):\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(net(X), y)  # 前向传播计算损失\n",
    "        l.mean().backward()  # 反向传播计算梯度\n",
    "        optimizer.step()  # 更新参数\n",
    "    with torch.no_grad(): # 确保在评估期间不计算梯度\n",
    "        train_l = loss(net(features), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 保存模型\n",
    "\n",
    "### 7.1 保存张量\n",
    "使用load和save函数分别读写即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')\n",
    "\n",
    "x2 = torch.load('x-file')\n",
    "x2\n",
    "\n",
    "# 存储张量列表\n",
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)\n",
    "\n",
    "# 字典映射的张量\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)\n",
    "\n",
    "torch.save(net.state_dict(), 'mlp.params')\n",
    "\n",
    "# 为了恢复模型，我们实例化了原始多层感知机模型的一个备份。 \n",
    "# 这里我们不需要随机初始化模型参数，而是直接读取文件中存储的参数。\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()\n",
    "\n",
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
